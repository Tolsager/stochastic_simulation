import matplotlib.pyplot as plt
import numpy as np
import time

from scipy.stats import geom
from discrete_sampling_functions import direct_sampling, rejection_sampling, alias_sampling


def kl_divergence(p: list, q: list) -> float:
    return np.sum([p[i]*np.log(p[i]/q[i]) for i in range(len(p))])


if __name__ == '__main__':
    TIME = True
    p = 0.3
    n = 10000

    unif_rvs = np.random.random(n)

    # Transform the uniforms random variables to geometric random variables
    geom_rvs = np.floor(np.log(unif_rvs)/np.log(1-p)) + 1

    fig, ax = plt.subplots(1, 2)

    ax[0].hist(geom_rvs, bins=range(1, int(max(geom_rvs))+1), density=True)
    ax[1].hist(geom.rvs(p, size=n), bins=range(1, int(max(geom_rvs))+1), density=True)
    plt.show()

    p = [7/48, 5/48, 1/8, 1/16, 1/4, 5/16]

    # Generate the six-point distribution
    fig, ax = plt.subplots(1, 3, figsize=(13, 5))

    start = time.perf_counter() if TIME else None
    direct_six_point_dist = direct_sampling(unif_rvs, p)
    if start is not None:
        print(f"Direct sampling took {time.perf_counter() - start:.2f} seconds")

    start = time.perf_counter() if TIME else None
    rejection_six_point_dist = rejection_sampling(p, n)
    if start is not None:
        print(f"Rejection sampling took {time.perf_counter() - start:.2f} seconds")

    start = time.perf_counter() if TIME else None
    alias_six_point_dist = alias_sampling(p, n)
    if start is not None:
        print(f"Alias sampling took {time.perf_counter() - start:.2f} seconds")

    ax[0].hist(direct_six_point_dist, bins=range(1, 8), density=True, width=0.9, align='mid', rwidth=0.9)
    ax[1].hist(rejection_six_point_dist, bins=range(1, 8), density=True, width=0.9, align='mid', rwidth=0.9)
    ax[2].hist(alias_six_point_dist, bins=range(1, 8), density=True, width=0.9, align='mid', rwidth=0.9)
    plt.show()

    # Compute the KL divergence for the distributions generated by the three methods and the true distribution
    direct_p = [direct_six_point_dist.count(i)/n for i in range(1, 7)]
    rejection_p = [rejection_six_point_dist.count(i)/n for i in range(1, 7)]
    alias_p = [alias_six_point_dist.count(i)/n for i in range(1, 7)]
    print(f"KL-divergence between Direct and Rejection: {kl_divergence(direct_p, rejection_p)}")
    print(f"KL-divergence between Direct and Alias: {kl_divergence(direct_p, alias_p)}")
    print(f"KL-divergence between Rejection and Alias: {kl_divergence(rejection_p, alias_p)}")
    print(f"KL-divergence between True and Direct: {kl_divergence(p, direct_p)}")
    print(f"KL-divergence between True and Rejection: {kl_divergence(p, rejection_p)}")
    print(f"KL-divergence between True and Alias: {kl_divergence(p, alias_p)}")


